# chdb-ds é¡¹ç›®æ·±åº¦è¯„å®¡æŠ¥å‘Š

ä½œä¸ºä¸€åä¸¥è°¨çš„æ•°æ®ç§‘å­¦å®¶ï¼Œæˆ‘å¯¹è¿™ä¸ªé¡¹ç›®è¿›è¡Œäº†æ·±å…¥çš„æµ‹è¯•å’Œè¯„å®¡ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„å‘ç°ï¼š

## ğŸ¯ æ€»ä½“è¯„ä»·

**ä¼˜ç‚¹:**
- âœ… æ ¸å¿ƒåŠŸèƒ½å·¥ä½œè‰¯å¥½
- âœ… Pandas API å…¼å®¹æ€§ç¡®å®å¾ˆå¼º (180+ æ–¹æ³•)
- âœ… ä¸å¯å˜æ€§è®¾è®¡åˆç†
- âœ… SQL ç”Ÿæˆæ­£ç¡®
- âœ… é”™è¯¯æ¶ˆæ¯æ¸…æ™°æœ‰ç”¨

**ä¸»è¦é—®é¢˜:**
- âš ï¸ README æ–‡æ¡£å­˜åœ¨è¯¯å¯¼æ€§å†…å®¹
- âš ï¸ å»¶è¿Ÿæ‰§è¡Œ(Lazy Execution)è¡Œä¸ºæœªå……åˆ†è¯´æ˜
- âš ï¸ API ä¸ä¸€è‡´æ€§
- âš ï¸ ç¼ºå°‘é‡è¦çš„ä½¿ç”¨ç¤ºä¾‹å’Œæœ€ä½³å®è·µ

---

## ğŸ“‹ å‘ç°çš„é—®é¢˜ (Issues Found)

### 1. ğŸ”´ README æ–‡æ¡£é—®é¢˜

#### 1.1 connect() çš„è¯¯å¯¼æ€§è¯´æ˜

**é—®é¢˜ä½ç½®:** README ç¬¬ 43-44 è¡Œ

```python
ds = DataStore.uri("/path/to/data.csv")
ds.connect()  # <-- è¿™è¡Œä¸æ˜¯å¿…éœ€çš„!
result = ds.select("*").filter(ds.age > 18).execute()
```

**å®é™…æƒ…å†µ:**
- `connect()` è°ƒç”¨æ˜¯å¯é€‰çš„ï¼Œä¸è°ƒç”¨ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ
- README å…¶ä»–åœ°æ–¹çš„ç¤ºä¾‹éƒ½æ²¡æœ‰è°ƒç”¨ `connect()`
- è¿™ä¼šè®©æ–°ç”¨æˆ·å›°æƒ‘ï¼šåˆ°åº•éœ€ä¸éœ€è¦è°ƒç”¨ connect()?

**å»ºè®®:**
- è¦ä¹ˆç§»é™¤ `ds.connect()` è¿™è¡Œ
- è¦ä¹ˆæ˜ç¡®è¯´æ˜å®ƒæ˜¯å¯é€‰çš„ï¼Œå¹¶è§£é‡Šä»€ä¹ˆæ—¶å€™éœ€è¦è°ƒç”¨

#### 1.2 åˆ—èµ‹å€¼è¡Œä¸ºæœªå……åˆ†è¯´æ˜

**é—®é¢˜ä½ç½®:** README ç¬¬ 179-180 è¡Œ

```python
# Column assignment with functions
ds['upper_name'] = ds['name'].str.upper()
ds['age_group'] = ds['age'] // 10 * 10
```

**å®é™…é—®é¢˜:**
åˆ—èµ‹å€¼æ˜¯ **å»¶è¿Ÿæ‰§è¡Œ** çš„,ä½† README æ²¡æœ‰è¯´æ˜è¿™ä¸€ç‚¹ã€‚

**æµ‹è¯•ç»“æœ:**
```python
ds['upper_name'] = ds['name'].str.upper()
print(ds.select('*').to_sql())
# SQL: SELECT * FROM file('/tmp/test.csv') AS "test"
# âŒ æ–°åˆ—ä¸åœ¨ SQL ä¸­!

result = ds.select('*').to_df()
print(result.columns)
# âœ“ ä½†æ˜¯ç»“æœä¸­æœ‰ 'upper_name' åˆ—!
```

**ç”¨æˆ·å›°æƒ‘:**
- SQL ä¸­çœ‹ä¸åˆ°æ–°åˆ—ï¼Œä½†æ‰§è¡Œç»“æœä¸­æœ‰
- è¿™æ˜¯å› ä¸ºåˆ—èµ‹å€¼è¢«è®°å½•ä¸º lazy operation
- README åº”è¯¥æ˜ç¡®è¯´æ˜è¿™ç§è¡Œä¸º

**å»ºè®®:**
åœ¨ README çš„åˆ—èµ‹å€¼éƒ¨åˆ†æ·»åŠ è¯´æ˜:
```python
# Column assignment is LAZY - it's recorded and applied during execution
ds['upper_name'] = ds['name'].str.upper()

# The SQL won't show the new column yet
print(ds.to_sql())  # Won't include upper_name

# But execution results will have it
result = ds.to_df()  # Will include upper_name column
```

### 2. âš ï¸ API ä¸ä¸€è‡´æ€§

#### 2.1 URI vs Factory Method ç”Ÿæˆä¸åŒçš„ SQL

**æµ‹è¯•ä»£ç :**
```python
# æ–¹å¼ 1: URI
ds1 = DataStore.uri('/tmp/test.csv')
print(ds1.select('*').to_sql())
# Output: SELECT * FROM file('/tmp/test.csv', 'CSV') AS "test"

# æ–¹å¼ 2: from_file
ds2 = DataStore.from_file('/tmp/test.csv')
print(ds2.select('*').to_sql())
# Output: SELECT * FROM file('/tmp/test.csv') AS "test"
```

**å·®å¼‚:**
- URI æ–¹å¼æ˜¾å¼æ·»åŠ  `'CSV'` æ ¼å¼å‚æ•°
- from_file æ–¹å¼ä¸æ·»åŠ æ ¼å¼å‚æ•°

**å½±å“:**
- åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½å¯¼è‡´ä¸åŒçš„è¡Œä¸º
- ç”¨æˆ·æœŸæœ›ä¸¤ç§æ–¹å¼åº”è¯¥ç­‰ä»·

**å»ºè®®:**
- ä¿æŒä¸€è‡´æ€§ï¼Œè¦ä¹ˆéƒ½åŠ æ ¼å¼ï¼Œè¦ä¹ˆéƒ½ä¸åŠ 
- æˆ–è€…åœ¨æ–‡æ¡£ä¸­æ˜ç¡®è¯´æ˜å·®å¼‚

### 3. ğŸŸ¡ æ–‡æ¡£å®Œæ•´æ€§é—®é¢˜

#### 3.1 å»¶è¿Ÿæ‰§è¡Œæ–‡æ¡£ç¼ºå¤±

README æåˆ° "Mixed Execution Engine" å’Œæ··åˆ SQL/Pandas æ“ä½œ,ä½†æ²¡æœ‰æ¸…æ¥šåœ°è§£é‡Š:

1. **ä»€ä¹ˆæ“ä½œæ˜¯å»¶è¿Ÿçš„?**
   - åˆ—èµ‹å€¼ (`ds['col'] = ...`)
   - å…¶ä»– Pandas æ“ä½œ?

2. **ä»€ä¹ˆæ“ä½œæ˜¯ç«‹å³çš„?**
   - `select()`, `filter()` (æ„å»º SQL)
   - `execute()`, `to_df()` (æ‰§è¡ŒæŸ¥è¯¢)

3. **å¦‚ä½•æŸ¥çœ‹å»¶è¿Ÿæ“ä½œé˜Ÿåˆ—?**
   - æ²¡æœ‰æä¾›æ–¹æ³•è®©ç”¨æˆ·æŸ¥çœ‹å¾…æ‰§è¡Œçš„ lazy ops

**å»ºè®®:** æ·»åŠ ä¸€ä¸ª "Execution Model" ç« èŠ‚,æ¸…æ™°è¯´æ˜:
```markdown
## Execution Model

chdb-ds uses a mixed execution model:

1. **SQL-building operations** (lazy, build the query):
   - `select()`, `filter()`, `groupby()`, `sort()`, etc.
   - Returns a new DataStore instance (immutable)
   - Does NOT execute the query

2. **Lazy operations** (recorded, executed during materialization):
   - Column assignment: `ds['new_col'] = expr`
   - Some pandas operations (TBD: list them)

3. **Execution operations** (trigger execution):
   - `execute()` / `exec()`: Returns QueryResult
   - `to_df()`: Returns pandas DataFrame
   - `to_dict()`: Returns list of dictionaries
   - Accessing properties like `shape`, `columns`, etc.

Example:
```python
ds = DataStore.from_file('data.csv')
ds = ds.select('*').filter(ds.age > 18)  # Lazy, builds SQL
ds['age_group'] = ds['age'] // 10 * 10   # Lazy, recorded

# Nothing executed yet!

result = ds.to_df()  # NOW it executes:
                     # 1. Runs the SQL query
                     # 2. Applies lazy operations
                     # 3. Returns the result
```
```

#### 3.2 ç¼ºå°‘æ€§èƒ½æœ€ä½³å®è·µ

README å±•ç¤ºäº†åŠŸèƒ½,ä½†æ²¡æœ‰å‘Šè¯‰ç”¨æˆ·:

1. **ä»€ä¹ˆæ—¶å€™ç”¨ SQL (chDB)?**
2. **ä»€ä¹ˆæ—¶å€™ç”¨ Pandas?**
3. **å¦‚ä½•ä¼˜åŒ–æ€§èƒ½?**

**ç¤ºä¾‹ç¼ºå¤±:**
```python
# âŒ Bad: Forces materialization early
df = ds.to_df()  # Loads all data
df = df[df['age'] > 18]  # Filters in pandas
result = df[['name', 'age']]

# âœ“ Good: Push down to SQL
result = ds.select('name', 'age').filter(ds.age > 18).to_df()
```

### 4. ğŸ”´ æ½œåœ¨ Bug / è®¾è®¡ç¼ºé™·

#### 4.1 æµ‹è¯•å¤±è´¥

è¿è¡Œé¡¹ç›®çš„æµ‹è¯•å¥—ä»¶å‘ç°å¤šä¸ªå¤±è´¥:
```
tests/test_arithmetic.py::ArithmeticExecutionTests::test_addition_execution FAILED
tests/test_arithmetic.py::ArithmeticExecutionTests::test_arithmetic_in_where_execution FAILED
tests/test_arithmetic.py::ArithmeticExecutionTests::test_complex_arithmetic_execution FAILED
...
```

è¿™è¡¨æ˜ç®—æœ¯è¿ç®—çš„æ‰§è¡Œå­˜åœ¨é—®é¢˜ã€‚

**å»ºè®®:** ä¿®å¤è¿™äº›æµ‹è¯•,ç¡®ä¿åŸºæœ¬åŠŸèƒ½æ­£å¸¸ã€‚

#### 4.2 åˆ—é€‰æ‹©çš„æ··æ·†

README ä¸­æ··ç”¨äº†ä¸¤ç§åˆ—é€‰æ‹©æ–¹å¼:
```python
ds.age > 18           # ç”¨äº filter
ds['name'].str.upper()  # ç”¨äºå­—ç¬¦ä¸²å‡½æ•°
```

è¿™ä¸¤ç§æ–¹å¼è¿”å›çš„éƒ½æ˜¯ `ColumnExpr`, ä½†ç”¨æˆ·å¯èƒ½ä¸æ¸…æ¥š:
- ä»€ä¹ˆæ—¶å€™ç”¨ `ds.column`?
- ä»€ä¹ˆæ—¶å€™ç”¨ `ds['column']`?
- å®ƒä»¬æœ‰åŒºåˆ«å—?

**æµ‹è¯•ç»“æœ:** å®ƒä»¬æ˜¯ç­‰ä»·çš„,ä½†è¿™åº”è¯¥åœ¨æ–‡æ¡£ä¸­è¯´æ˜ã€‚

---

## ğŸ¨ è®¾è®¡é—®é¢˜å’Œæ”¹è¿›å»ºè®®

### 1. åˆ—èµ‹å€¼çš„è®¾è®¡é—®é¢˜

**å½“å‰è¡Œä¸º:**
```python
ds['new_col'] = ds['old_col'] * 2
# è¿™ä¿®æ”¹äº† ds (ä¸ç¬¦åˆä¸å¯å˜æ€§åŸåˆ™!)
```

**é—®é¢˜:**
- README å£°ç§° "Immutable Operations"
- ä½†æ˜¯ `__setitem__` æ˜¯å°±åœ°ä¿®æ”¹ `_lazy_ops` åˆ—è¡¨
- è¿™è¿åäº†ä¸å¯å˜æ€§æ‰¿è¯º

**å»ºè®®æ–¹æ¡ˆ 1:** ä½¿ç”¨ `assign()` (æ›´ç¬¦åˆ pandas é£æ ¼)
```python
# è¿”å›æ–°çš„ DataStore
ds2 = ds.assign(new_col=ds['old_col'] * 2)
```

**å»ºè®®æ–¹æ¡ˆ 2:** ä½¿ `__setitem__` ä¹Ÿä¸å¯å˜
```python
ds2 = ds  # Copy-on-write
ds2['new_col'] = ds2['old_col'] * 2  # Creates a copy
```

### 2. ç¼ºå°‘è°ƒè¯•åŠŸèƒ½

**å½“å‰é—®é¢˜:**
- æ²¡æœ‰ç®€å•çš„æ–¹æ³•æŸ¥çœ‹å°†è¦æ‰§è¡Œçš„ SQL
- æ²¡æœ‰æ–¹æ³•æŸ¥çœ‹å»¶è¿Ÿæ“ä½œé˜Ÿåˆ—
- æ²¡æœ‰ EXPLAIN åŠŸèƒ½ (å®é™…ä¸Šæœ‰,ä½† README æ²¡æ)

**å»ºè®®:** æ·»åŠ åˆ° README:
```python
# See the SQL that will be executed
print(ds.to_sql())

# See the execution plan
ds.explain()

# See lazy operations (if any)
print(ds._lazy_ops)  # Document this
```

### 3. ç¼ºå°‘æ•°æ®æ¢ç´¢åŠŸèƒ½

Pandas ç”¨æˆ·ä¹ æƒ¯äº†å¿«é€Ÿæ•°æ®æ¢ç´¢:
```python
df.head()
df.info()
df.describe()
df.shape
```

chdb-ds æ”¯æŒè¿™äº›,ä½†æ²¡æœ‰åœ¨ Quick Start ä¸­å±•ç¤º!

**å»ºè®®:** åœ¨ Quick Start åé¢æ·»åŠ ä¸€ä¸ª "Data Exploration" éƒ¨åˆ†:
```python
# Quick data exploration
ds = DataStore.from_file('data.csv')

# View first few rows
print(ds.head())

# Get dataset info
print(ds.shape)      # (1000, 5)
print(ds.columns)    # ['id', 'name', 'age', 'city', 'value']
print(ds.dtypes)     # Column types

# Statistical summary
print(ds.describe())

# Memory usage and info
ds.info()
```

---

## ğŸ“Š README ç»“æ„æ”¹è¿›å»ºè®®

å½“å‰ README çš„é—®é¢˜:
1. å¤ªé•¿,é‡è¦ä¿¡æ¯è¢«æ·¹æ²¡
2. Quick Start ä¸å¤Ÿ"Quick"
3. é«˜çº§ç‰¹æ€§æ··åœ¨åŸºç¡€ç‰¹æ€§ä¸­
4. ç¼ºå°‘"ä» Pandas è¿ç§»"æŒ‡å—

### å»ºè®®çš„ README ç»“æ„:

```markdown
# DataStore

[Badges]

> âš ï¸ **EXPERIMENTAL**

ç®€çŸ­çš„ä¸€å¥è¯æè¿°

## ğŸš€ Quick Start (5 minutes)

### Installation
```bash
pip install chdb-ds
```

### Your First Query
```python
from datastore import DataStore

# 1. Load data
ds = DataStore.from_file('data.csv')

# 2. Explore
print(ds.head())        # View data
print(ds.shape)         # (1000, 5)

# 3. Query
result = (ds
    .select('name', 'age', 'city')
    .filter(ds.age > 18)
    .sort('name')
    .limit(10)
    .to_df())  # Returns pandas DataFrame
```

## ğŸ’¡ Key Concepts

### SQL vs Pandas Operations
[Explain when to use which]

### Lazy vs Eager Execution
[Explain the execution model]

### Immutability
[Explain method chaining]

## ğŸ“š Guides

- [From Pandas to DataStore](docs/PANDAS_MIGRATION.md)
- [Data Sources](docs/DATA_SOURCES.md)
- [Function Reference](docs/FUNCTIONS.md)
- [Performance Tips](docs/PERFORMANCE.md)

## ğŸ¯ Common Tasks

### Data Loading
[Most common scenarios]

### Data Exploration
[Head, describe, info, etc.]

### Data Transformation
[Filter, select, groupby, join]

### Working with Multiple Sources
[Joins across sources]

## ğŸ“– Full Documentation
[Link to detailed docs]
```

---

## ğŸ› å…·ä½“çš„ Bug æŠ¥å‘Š

### Bug 1: ç®—æœ¯è¿ç®—æ‰§è¡Œå¤±è´¥

**æµ‹è¯•å¤±è´¥:**
```
tests/test_arithmetic.py::ArithmeticExecutionTests - FAILED
```

**é‡ç°æ­¥éª¤:**
éœ€è¦æŸ¥çœ‹å…·ä½“çš„æµ‹è¯•ä»£ç ,ä½†æµ‹è¯•å¥—ä»¶æ˜¾ç¤ºç®—æœ¯è¿ç®—æ‰§è¡Œå­˜åœ¨é—®é¢˜ã€‚

**ä¼˜å…ˆçº§:** HIGH (åŸºæœ¬åŠŸèƒ½)

### Bug 2: URI å’Œ from_file çš„ SQL ç”Ÿæˆä¸ä¸€è‡´

**é‡ç°:**
```python
ds1 = DataStore.uri('/tmp/test.csv')
ds2 = DataStore.from_file('/tmp/test.csv')
assert ds1.to_sql() == ds2.to_sql()  # FAILS
```

**ä¼˜å…ˆçº§:** MEDIUM

---

## âœ… éªŒè¯çš„åŠŸèƒ½ (What Works Well)

1. âœ… **åŸºæœ¬æ•°æ®åŠ è½½å’ŒæŸ¥è¯¢**
   - `from_numbers()`, `from_file()` å·¥ä½œæ­£å¸¸
   - `select()`, `filter()`, `limit()` ç”Ÿæˆæ­£ç¡®çš„ SQL

2. âœ… **Pandas API å…¼å®¹æ€§**
   - æ‰€æœ‰ README ä¸­æåˆ°çš„æ–¹æ³•éƒ½å­˜åœ¨
   - `head()`, `tail()`, `describe()`, `shape`, `columns`, etc.

3. âœ… **ä¸å¯å˜æ€§**
   - æ“ä½œè¿”å›æ–°å®ä¾‹,ä¸ä¿®æ”¹åŸå®ä¾‹
   - å¯ä»¥å®‰å…¨åœ°è¿›è¡Œæ–¹æ³•é“¾å¼è°ƒç”¨

4. âœ… **é”™è¯¯æ¶ˆæ¯**
   - é”™è¯¯æ¶ˆæ¯æ¸…æ™°,åŒ…å«æœ‰ç”¨ä¿¡æ¯
   - æåˆ°ç›¸å…³çš„åˆ—åã€æ–‡ä»¶è·¯å¾„ç­‰

5. âœ… **å­—ç¬¦ä¸²å’Œæ—¥æœŸå‡½æ•°**
   - `.str` å’Œ `.dt` accessor å·¥ä½œæ­£å¸¸
   - SQL ç”Ÿæˆæ­£ç¡®

6. âœ… **æ··åˆæ‰§è¡Œ**
   - å¯ä»¥æ··åˆä½¿ç”¨ SQL å’Œ Pandas æ“ä½œ
   - `filter()` -> `assign()` -> `filter()` å·¥ä½œæ­£å¸¸

---

## ğŸ¯ ä¼˜å…ˆæ”¹è¿›å»ºè®®

### å¿…é¡»ä¿®å¤ (P0):
1. ä¿®å¤ç®—æœ¯è¿ç®—æ‰§è¡Œçš„æµ‹è¯•å¤±è´¥
2. æ˜ç¡®æ–‡æ¡£åŒ–å»¶è¿Ÿæ‰§è¡Œè¡Œä¸º
3. ä¿®å¤æˆ–æ–‡æ¡£åŒ– URI vs from_file çš„å·®å¼‚

### åº”è¯¥æ”¹è¿› (P1):
1. æ·»åŠ  "Execution Model" æ–‡æ¡£ç« èŠ‚
2. åœ¨ Quick Start ä¸­å±•ç¤ºæ•°æ®æ¢ç´¢åŠŸèƒ½
3. æ·»åŠ æ€§èƒ½æœ€ä½³å®è·µæŒ‡å—
4. ç§»é™¤æˆ–è¯´æ˜ `connect()` çš„å¿…è¦æ€§

### å¯ä»¥è€ƒè™‘ (P2):
1. é‡æ„ README ç»“æ„,ä½¿å…¶æ›´æ˜“è¯»
2. æ·»åŠ  "From Pandas" è¿ç§»æŒ‡å—
3. æ·»åŠ æ›´å¤šå®é™…ä½¿ç”¨æ¡ˆä¾‹
4. æ”¹è¿›åˆ—èµ‹å€¼çš„ API (è€ƒè™‘ä¸å¯å˜æ€§)

---

## ğŸ“ˆ æ€§èƒ½æµ‹è¯•å»ºè®®

å»ºè®®æ·»åŠ ä»¥ä¸‹æ€§èƒ½æµ‹è¯•:

1. **å¤§æ–‡ä»¶åŠ è½½æ€§èƒ½**
   ```python
   # 1M rows, 10 columns
   ds = DataStore.from_file('large_file.parquet')
   %timeit ds.select('*').filter(ds.value > 1000).to_df()
   ```

2. **SQL vs Pandas è¿‡æ»¤æ€§èƒ½**
   ```python
   # SQL filtering (should be faster)
   %timeit ds.filter(ds.value > 1000).to_df()

   # Pandas filtering (should be slower)
   %timeit ds.to_df()[lambda x: x['value'] > 1000]
   ```

3. **Join æ€§èƒ½**
   ```python
   # Join across different sources
   %timeit ds1.join(ds2, on='id').to_df()
   ```

---

## ğŸ“ æ–‡æ¡£æ”¹è¿›çš„å…·ä½“å»ºè®®

### 1. åœ¨ README å¼€å¤´æ·»åŠ  "When to Use" ç« èŠ‚

```markdown
## When to Use chdb-ds

chdb-ds is ideal for:
- âœ… Analyzing large datasets that don't fit in pandas
- âœ… Querying multiple data sources (files, databases, cloud storage)
- âœ… Pushing down filters and aggregations to SQL for performance
- âœ… Teams familiar with pandas who want SQL performance

Not ideal for:
- âŒ Small datasets (< 100MB) - just use pandas
- âŒ Complex pandas operations not translatable to SQL
- âŒ Real-time streaming data
```

### 2. æ·»åŠ  "Common Pitfalls" ç« èŠ‚

```markdown
## Common Pitfalls

### 1. Materializing Too Early
```python
# âŒ Bad: Loads all data into memory first
df = ds.to_df()
result = df[df['age'] > 18]

# âœ“ Good: Filter in SQL first
result = ds.filter(ds.age > 18).to_df()
```

### 2. Forgetting Lazy Evaluation
```python
ds['new_col'] = ds['old_col'] * 2
print(ds.to_sql())  # Won't show new_col yet!
# Use .to_df() to materialize
```

### 3. Using `and`/`or` Instead of `&`/`|`
```python
# âŒ Wrong: Python operators don't work
ds.filter((ds.age > 18) and (ds.age < 65))

# âœ“ Correct: Use bitwise operators
ds.filter((ds.age > 18) & (ds.age < 65))
```
```

### 3. æ·»åŠ æ€§èƒ½æç¤º

```markdown
## Performance Tips

1. **Filter Early, Select Late**
   ```python
   # Filter first to reduce data
   ds.filter(conditions).select(columns)
   ```

2. **Use Parquet for Large Files**
   - Faster than CSV
   - Column-oriented storage
   - Built-in compression

3. **Push Aggregations to SQL**
   ```python
   # âœ“ Fast: SQL aggregation
   ds.groupby('category').agg({'value': 'sum'})

   # âŒ Slow: Load then aggregate in pandas
   ds.to_df().groupby('category')['value'].sum()
   ```

4. **Use Format Settings for S3/Cloud Data**
   ```python
   ds.with_format_settings(
       input_format_parquet_filter_push_down=1
   )
   ```
```

---

## ğŸ“ æ€»ç»“

### è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ½œåŠ›çš„é¡¹ç›®!

**æ ¸å¿ƒåŠŸèƒ½æ‰å®:**
- SQL ç”Ÿæˆæ­£ç¡®
- Pandas API å…¼å®¹æ€§å¥½
- ä¸å¯å˜æ€§è®¾è®¡åˆç†
- æ”¯æŒå¤šç§æ•°æ®æº

**ä¸»è¦éœ€è¦æ”¹è¿›çš„æ˜¯æ–‡æ¡£:**
- å»¶è¿Ÿæ‰§è¡Œè¡Œä¸ºéœ€è¦æ˜ç¡®è¯´æ˜
- éœ€è¦æ›´å¥½çš„ä½¿ç”¨æŒ‡å—å’Œæœ€ä½³å®è·µ
- README ç»“æ„éœ€è¦ä¼˜åŒ–,ä½¿å…¶æ›´æ˜“è¯»
- éœ€è¦"ä» Pandas è¿ç§»"çš„æŒ‡å¯¼

**å‘ç°çš„ Bug:**
- ç®—æœ¯è¿ç®—æ‰§è¡Œæµ‹è¯•å¤±è´¥ (éœ€è¦ä¿®å¤)
- URI vs from_file çš„ç»†å¾®å·®å¼‚ (éœ€è¦æ–‡æ¡£åŒ–æˆ–ä¿®å¤)

### æ¨èç»™ç”¨æˆ·å—?

**å¯¹äºæ—©æœŸé‡‡ç”¨è€…:** âœ… YES
- æ ¸å¿ƒåŠŸèƒ½å¯ç”¨
- æ€§èƒ½åº”è¯¥ä¸é”™
- ä½†éœ€è¦æ³¨æ„æ–‡æ¡£ä¸å®Œæ•´

**å¯¹äºç”Ÿäº§ç¯å¢ƒ:** âš ï¸ è°¨æ…
- ç­‰å¾…æ›´å¤šæµ‹è¯•é€šè¿‡
- ç­‰å¾…æ–‡æ¡£å®Œå–„
- ç­‰å¾… Beta ç‰ˆæœ¬

---

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœé¡¹ç›®ä½œè€…æƒ³è¦è®¨è®ºè¿™äº›å‘ç°,æˆ‘å¾ˆä¹æ„è¯¦ç»†è¯´æ˜ä»»ä½•éƒ¨åˆ†ã€‚

ä½œä¸ºä¸€åæ•°æ®ç§‘å­¦å®¶,æˆ‘è®¤ä¸ºè¿™ä¸ªé¡¹ç›®è§£å†³äº†ä¸€ä¸ªçœŸå®çš„ç—›ç‚¹:
**åœ¨ Pandas çš„æ˜“ç”¨æ€§å’Œ SQL çš„æ€§èƒ½ä¹‹é—´æ‰¾åˆ°å¹³è¡¡**ã€‚

åŠ æ²¹! ğŸ’ª
