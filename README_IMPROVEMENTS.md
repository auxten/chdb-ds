# README æ”¹è¿›å»ºè®® - å¯ç›´æ¥åº”ç”¨

## ğŸ”´ å…³é”®é—®é¢˜ä¿®å¤

### 1. ç§»é™¤æˆ–è¯´æ˜ connect() çš„å¿…è¦æ€§

**å½“å‰ (ç¬¬ 43-44 è¡Œ):**
```python
# Local files - format auto-detected from extension
ds = DataStore.uri("/path/to/data.csv")
ds.connect()  # â† è¿™è¡Œé€ æˆå›°æƒ‘
result = ds.select("*").filter(ds.age > 18).execute()
```

**å»ºè®®ä¿®æ”¹ä¸º:**
```python
# Local files - format auto-detected from extension
ds = DataStore.uri("/path/to/data.csv")
# Note: connect() is optional and called automatically during execution
result = ds.select("*").filter(ds.age > 18).execute()
```

æˆ–è€…ç›´æ¥åˆ é™¤ `ds.connect()` è¿™è¡Œ,å› ä¸ºå…¶ä»–æ‰€æœ‰ç¤ºä¾‹éƒ½æ²¡æœ‰ç”¨å®ƒã€‚

---

### 2. æ·»åŠ å»¶è¿Ÿæ‰§è¡Œè¯´æ˜

**åœ¨ç¬¬ 179 è¡Œä¹‹å‰æ·»åŠ :**

```markdown
### Column Assignment and Lazy Evaluation

Column assignment operations are **lazy** - they are recorded and executed when you materialize the data:

```python
ds = DataStore.from_file("data.csv")

# Record a new column (lazy - not executed yet)
ds['upper_name'] = ds['name'].str.upper()
ds['age_group'] = ds['age'] // 10 * 10

# The SQL won't show new columns yet
print(ds.select('*').to_sql())
# Output: SELECT * FROM file('data.csv') AS "data"

# But execution results WILL include them
result = ds.select('*').to_df()
print(result.columns)
# Output: ['id', 'name', 'age', 'upper_name', 'age_group']

# You can also explicitly select the new columns
result = ds.select('name', 'upper_name').to_df()
```

**Important Notes:**
- Column assignment modifies the DataStore in-place (not immutable)
- The assignment is recorded and applied during data materialization
- Use `assign()` for immutable column creation:
  ```python
  ds2 = ds.assign(upper_name=lambda x: x['name'].str.upper())
  ```
```

---

### 3. åœ¨ Quick Start åæ·»åŠ æ•°æ®æ¢ç´¢ç« èŠ‚

**åœ¨ç¬¬ 126 è¡Œ (execute() ç¤ºä¾‹å) æ·»åŠ :**

```markdown
### Data Exploration

DataStore provides familiar pandas-like methods for quick data exploration:

```python
from datastore import DataStore

# Load data
ds = DataStore.from_file("sales.csv")

# Quick peek at data
print(ds.head())        # First 5 rows
print(ds.tail(3))       # Last 3 rows
print(ds.sample(10))    # Random 10 rows

# Dataset info
print(ds.shape)         # (1000, 7) - rows and columns
print(ds.columns)       # ['id', 'product', 'price', ...]
print(ds.dtypes)        # Column data types

# Statistical summary
print(ds.describe())    # Count, mean, std, min, max, percentiles

# Detailed info
ds.info()              # Memory usage, non-null counts, dtypes

# Quick statistics
print(ds['price'].mean())    # Average price
print(ds['quantity'].sum())  # Total quantity
print(ds['customer_id'].count_distinct())  # Unique customers
```

**Tip:** These operations execute the query and return results. For large datasets,
consider adding filters first to reduce data size:

```python
# Better for large datasets
ds.filter(ds.date >= '2024-01-01').describe()
```
```

---

## ğŸŸ¡ é‡è¦çš„æ¾„æ¸…å’Œæ”¹è¿›

### 4. æ·»åŠ  "Execution Model" ç« èŠ‚

**å»ºè®®åœ¨ "Design Philosophy" ä¹‹å‰æ·»åŠ æ–°ç« èŠ‚:**

```markdown
## Execution Model

Understanding when operations execute is key to using DataStore effectively:

### 1. Query Building (Lazy)

These operations build the SQL query but don't execute it:

```python
ds = DataStore.from_file("data.csv")
ds = ds.select("name", "age")           # Lazy
ds = ds.filter(ds.age > 18)              # Lazy
ds = ds.sort("name")                     # Lazy
ds = ds.limit(10)                        # Lazy

# Nothing executed yet! Just building the query.
print(ds.to_sql())  # Shows the SQL that will be executed
```

### 2. Lazy Operations (Recorded)

Column assignments are recorded and applied during execution:

```python
ds['new_col'] = ds['old_col'] * 2    # Recorded, not executed
```

### 3. Execution (Eager)

These trigger actual query execution:

```python
# Execute and get different result formats
result = ds.execute()    # Returns QueryResult object
df = ds.to_df()          # Returns pandas DataFrame
records = ds.to_dict()   # Returns list of dictionaries

# These also trigger execution
shape = ds.shape         # Executes to count rows/cols
cols = ds.columns        # Executes to get column names
stats = ds.describe()    # Executes and computes statistics
```

### Best Practice: Filter Early

Push filters to SQL for best performance:

```python
# âœ… Good: SQL filter (fast, processes less data)
result = ds.filter(ds.date >= '2024-01-01').to_df()

# âŒ Bad: Load everything, then filter in pandas (slow)
result = ds.to_df()
result = result[result['date'] >= '2024-01-01']
```

### Query Reuse

DataStore is immutable (except column assignment), so you can reuse query objects:

```python
base_query = ds.select("*").filter(ds.status == "active")

# Create different queries from the same base
recent = base_query.filter(ds.date >= '2024-01-01')
high_value = base_query.filter(ds.value > 1000)

# Each executes independently
recent_df = recent.to_df()
high_value_df = high_value.to_df()
```
```

---

### 5. æ·»åŠ  "Common Pitfalls" è­¦å‘Šæ¡†

**åœ¨ Quick Start ç»“æŸåæ·»åŠ :**

```markdown
> âš ï¸ **Common Pitfalls**
>
> 1. **Using `and`/`or` instead of `&`/`|` in conditions:**
>    ```python
>    # âŒ Wrong
>    ds.filter((ds.age > 18) and (ds.age < 65))
>
>    # âœ… Correct
>    ds.filter((ds.age > 18) & (ds.age < 65))
>    ```
>
> 2. **Forgetting to materialize after column assignment:**
>    ```python
>    ds['new_col'] = ds['old_col'] * 2
>    # Need to execute to see results
>    result = ds.to_df()  # or .execute()
>    ```
>
> 3. **Loading all data before filtering:**
>    ```python
>    # âŒ Loads everything into memory
>    df = ds.to_df()
>    filtered = df[df['value'] > 100]
>
>    # âœ… Filter in SQL first
>    filtered = ds.filter(ds.value > 100).to_df()
>    ```
```

---

### 6. æ”¹è¿› Quick Start çš„æµç¨‹

**å½“å‰é—®é¢˜:** Quick Start ç›´æ¥è·³åˆ° URI åˆ›å»º,ä½†å¤§å¤šæ•°ç”¨æˆ·å¯èƒ½æƒ³ä»æœ€ç®€å•çš„ä¾‹å­å¼€å§‹ã€‚

**å»ºè®®ç»“æ„:**

```markdown
## Quick Start

### Installation

```bash
pip install chdb-ds
```

### Your First Query (30 seconds)

```python
from datastore import DataStore

# Generate some test data
ds = DataStore.from_numbers(100)  # Creates 0-99

# Query with pandas-like syntax
result = (ds
    .select('*')
    .filter(ds.number > 50)
    .limit(5)
    .to_df())  # Returns pandas DataFrame

print(result)
#    number
# 0      51
# 1      52
# 2      53
# 3      54
# 4      55
```

### Real Data (1 minute)

```python
# Local CSV file
ds = DataStore.from_file("sales.csv")

# Explore
print(ds.head())       # Preview data
print(ds.shape)        # (10000, 5)

# Query
result = (ds
    .select("product", "revenue", "date")
    .filter(ds.revenue > 1000)
    .filter(ds.date >= "2024-01-01")
    .sort("revenue", ascending=False)
    .limit(10)
    .to_df())

print(result)
```

### URI-based Creation (Recommended for Complex Sources)

The easiest way to work with remote data sources is using URI strings...
[Keep existing URI content]
```

---

### 7. æ˜ç¡®è¯´æ˜ä¸¤ç§åˆ—è®¿é—®æ–¹å¼

**åœ¨ "Working with Expressions" ç« èŠ‚æ·»åŠ è¯´æ˜:**

```markdown
### Field Access

DataStore supports two equivalent ways to access columns:

```python
# Style 1: Attribute access (shorter, more readable)
ds.price
ds.age > 18

# Style 2: Dictionary access (works with any column name)
ds['price']
ds['age'] > 18

# Both return the same type (ColumnExpr) and generate identical SQL
```

**When to use which:**
- Use `ds.column` for clean, readable code when column names are valid Python identifiers
- Use `ds['column']` when:
  - Column name has spaces or special characters: `ds['customer name']`
  - Column name conflicts with methods: `ds['select']`, `ds['filter']`
  - Accessing columns dynamically: `ds[col_name]`
  - Using with string/date accessors: `ds['name'].str.upper()`

Both styles work with all operations:
```python
# These are equivalent
ds.select(ds.price * 1.1)
ds.select(ds['price'] * 1.1)

# These are equivalent
ds.filter(ds.age > 18)
ds.filter(ds['age'] > 18)
```
```

---

## ğŸ“Š å…¶ä»–å»ºè®®çš„æ”¹è¿›

### 8. æ·»åŠ æ€§èƒ½æç¤ºæ¡†

**åœ¨ "Format Settings" ä¹‹åæ·»åŠ :**

```markdown
## Performance Tips

### 1. Push Operations to SQL

DataStore's power comes from executing operations in SQL (chDB). Keep operations in the SQL layer as long as possible:

```python
# âœ… Excellent: Everything in SQL
result = (ds
    .select('category', 'product', 'revenue')
    .filter(ds.date >= '2024-01-01')
    .filter(ds.revenue > 1000)
    .groupby('category', 'product')
    .agg({'revenue': 'sum'})
    .sort('revenue', ascending=False)
    .limit(100)
    .to_df())

# âŒ Poor: Materializes too early
df = ds.to_df()  # Loads ALL data into memory
df = df[df['date'] >= '2024-01-01']
df = df[df['revenue'] > 1000]
# ...
```

### 2. Select Only What You Need

```python
# âœ… Select specific columns
ds.select('id', 'name', 'value')

# âŒ Select everything then subset
df = ds.select('*').to_df()
df = df[['id', 'name', 'value']]
```

### 3. Use Appropriate File Formats

- **CSV**: Human-readable, slow for large files
- **Parquet**: Best for large datasets (compressed, columnar)
- **JSON**: Flexible schema, moderate performance

```python
# Convert CSV to Parquet for better performance
ds_csv = DataStore.from_file("large_data.csv")
ds_csv.to_parquet("large_data.parquet")

# Much faster to read
ds_parquet = DataStore.from_file("large_data.parquet")
```

### 4. Optimize Cloud Storage Access

```python
# Enable filter pushdown for Parquet on S3
ds = (DataStore.from_s3("s3://bucket/data.parquet")
      .with_format_settings(
          input_format_parquet_filter_push_down=1,
          input_format_parquet_bloom_filter_push_down=1
      ))

# Now filters are pushed to S3, reducing data transfer
result = ds.filter(ds.date >= '2024-01-01').to_df()
```

### 5. Reuse Query Objects

```python
# Build base query once
base = ds.select('*').filter(ds.status == 'active')

# Reuse for different analyses
high_value = base.filter(ds.value > 1000).to_df()
recent = base.filter(ds.date >= '2024-01-01').to_df()
summary = base.groupby('category').agg({'value': 'sum'}).to_df()
```
```

---

### 9. æ”¹è¿› "Supported Data Sources" ç« èŠ‚çš„ç»„ç»‡

**å½“å‰é—®é¢˜:** å¤ªé•¿,åƒä»£ç åº“æµè§ˆ,ä¸åƒæ–‡æ¡£ã€‚

**å»ºè®®:** æ”¹ä¸ºè¡¨æ ¼å½¢å¼,æŠŠè¯¦ç»†ç¤ºä¾‹ç§»åˆ°å•ç‹¬çš„æ–‡æ¡£:

```markdown
## Supported Data Sources

DataStore supports 20+ data sources through a unified interface:

| Category | Sources | Example |
|----------|---------|---------|
| **Local Files** | CSV, Parquet, JSON, ORC, Avro, [80+ formats](https://clickhouse.com/docs/interfaces/formats) | `DataStore.from_file("data.csv")` |
| **Cloud Storage** | S3, GCS, Azure Blob, HDFS | `DataStore.from_s3("s3://bucket/data.parquet")` |
| **Databases** | MySQL, PostgreSQL, ClickHouse, MongoDB, SQLite, Redis | `DataStore.from_mysql(host, db, table)` |
| **Data Lakes** | Iceberg, Delta Lake, Hudi | `DataStore.from_delta("s3://bucket/table")` |
| **Other** | HTTP/HTTPS, Number generation, Random data | `DataStore.from_url("https://...")` |

### Quick Examples

**Local Files:**
```python
# Auto-detect format from extension
ds = DataStore.from_file("data.parquet")
ds = DataStore.from_file("data.csv")
```

**Cloud Storage:**
```python
# S3 with public access
ds = DataStore.from_s3("s3://bucket/data.parquet", nosign=True)

# With credentials
ds = DataStore.from_s3("s3://bucket/*.csv",
                       access_key_id="KEY",
                       secret_access_key="SECRET")
```

**Databases:**
```python
# MySQL
ds = DataStore.from_mysql("localhost:3306", "mydb", "users",
                          user="root", password="pass")

# PostgreSQL
ds = DataStore.from_postgresql("localhost:5432", "mydb", "users",
                               user="postgres", password="pass")
```

**Data Generation (for testing):**
```python
# Number sequence
ds = DataStore.from_numbers(1000)  # 0-999

# Random data
ds = DataStore.from_random(
    structure="id UInt32, name String, value Float64",
    random_seed=42
)
```

ğŸ“– **See [examples/examples_table_functions.py](examples/examples_table_functions.py) for comprehensive examples of all data sources.**
```

è¿™æ ·æ›´ç®€æ´,ç”¨æˆ·å¯ä»¥å¿«é€Ÿæ‰¾åˆ°ä»–ä»¬éœ€è¦çš„æ•°æ®æº,è¯¦ç»†ç¤ºä¾‹åœ¨å•ç‹¬çš„æ–‡ä»¶ä¸­ã€‚

---

## ğŸ“ å°çš„æ–‡å­—æ”¹è¿›

### 10. ä¿®æ­£ç¤ºä¾‹ä»£ç çš„ä¸€è‡´æ€§

**ç¬¬ 93 è¡Œ:**
```python
# å½“å‰
ds = DataStore.from_s3("s3://bucket/data.parquet", nosign=True)

# åº”è¯¥ä¿æŒå‚æ•°é¡ºåºä¸€è‡´,æˆ–è€…ä½¿ç”¨å‘½åå‚æ•°
ds = DataStore.from_s3("s3://bucket/data.parquet", nosign=True)
```

**ç¬¬ 233 è¡Œ:**
```python
# å½“å‰
ds.assign(new_col=lambda x: x['col1'] * 2)

# å»ºè®®æ·»åŠ è¯´æ˜è¿™æ˜¯ pandas é£æ ¼çš„ assign,è¿”å›æ–°çš„ DataStore
new_ds = ds.assign(new_col=lambda x: x['col1'] * 2)  # Returns new DataStore
```

---

## ğŸ¯ æ€»ç»“

### ä¼˜å…ˆçº§æ’åº:

**P0 (ç«‹å³ä¿®å¤):**
1. âœ… ç§»é™¤æˆ–è¯´æ˜ `connect()` è°ƒç”¨ (ç¬¬ 44 è¡Œ)
2. âœ… æ·»åŠ å»¶è¿Ÿæ‰§è¡Œè¯´æ˜ (åœ¨åˆ—èµ‹å€¼éƒ¨åˆ†)
3. âœ… æ·»åŠ  "Common Pitfalls" è­¦å‘Š

**P1 (å¼ºçƒˆå»ºè®®):**
4. âœ… æ·»åŠ  "Execution Model" ç« èŠ‚
5. âœ… åœ¨ Quick Start åæ·»åŠ æ•°æ®æ¢ç´¢ç¤ºä¾‹
6. âœ… æ”¹è¿› Quick Start çš„æµç¨‹

**P2 (æœ‰æ—¶é—´å†åš):**
7. âœ… æ·»åŠ æ€§èƒ½æç¤ºç« èŠ‚
8. âœ… ç®€åŒ– "Supported Data Sources" ç« èŠ‚
9. âœ… æ˜ç¡®è¯´æ˜ä¸¤ç§åˆ—è®¿é—®æ–¹å¼

---

## ğŸ“„ å…¶ä»–æ–‡æ¡£å»ºè®®

å»ºè®®åˆ›å»ºä»¥ä¸‹ç‹¬ç«‹æ–‡æ¡£:

1. **docs/MIGRATION.md** - ä» Pandas è¿ç§»æŒ‡å—
2. **docs/PERFORMANCE.md** - æ€§èƒ½ä¼˜åŒ–è¯¦ç»†æŒ‡å—
3. **docs/DATA_SOURCES.md** - æ‰€æœ‰æ•°æ®æºçš„è¯¦ç»†æ–‡æ¡£
4. **docs/EXECUTION_MODEL.md** - æ·±å…¥è§£é‡Šæ‰§è¡Œæ¨¡å‹
5. **docs/BEST_PRACTICES.md** - æœ€ä½³å®è·µæ±‡æ€»

è¿™æ ·å¯ä»¥ä¿æŒ README ç®€æ´,åŒæ—¶æä¾›æ·±å…¥çš„æ–‡æ¡£ç»™éœ€è¦çš„ç”¨æˆ·ã€‚
